{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ODSC_News_ABSA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIMNkfEZA59U",
        "colab_type": "code",
        "outputId": "0736a02c-f0d5-4519-a9c7-37360de5b068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import json \n",
        "import pandas as pd \n",
        "from pandas.io.json import json_normalize\n",
        "import types\n",
        "import fastai\n",
        "from fastai import *\n",
        "from fastai.text import * \n",
        "import numpy as np\n",
        "from functools import partial\n",
        "import re, io, os, collections, html\n",
        "\n",
        "\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')  \n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words(\"english\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5RlWts5BqRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create train and test datasets for fine-tuning language model from NLTK Reuters corpus\n",
        "\n",
        "def train_test_sets(documents):\n",
        "    train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
        "    test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
        "\n",
        "    train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
        "    test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
        "    \n",
        "    df_train_text = pd.DataFrame(train_docs, columns=['text'])\n",
        "    df_val_text = pd.DataFrame(test_docs, columns=['text'])\n",
        "    \n",
        "    train_labels = [reuters.categories(doc_id)[0] for doc_id in train_docs_id]\n",
        "    test_labels = [reuters.categories(doc_id)[0] for doc_id in test_docs_id]\n",
        "    \n",
        "    df_train_label = pd.DataFrame(train_labels, columns=['label'])\n",
        "    df_val_label = pd.DataFrame(test_labels, columns=['label'])\n",
        "    \n",
        "    df_train = pd.concat([df_train_label, df_train_text], axis=1)\n",
        "    df_val = pd.concat([df_val_label, df_val_text], axis=1)\n",
        "    \n",
        "    return df_train, df_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFvowLBBC4tQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train, df_val = train_test_sets(reuters.fileids())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMcav_9Flg7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fine tune the language model pre-trained on Wikipedia text using NLTK Reuters data\n",
        "def get_finetuned_LM():\n",
        "    data_lm = TextLMDataBunch.from_df(train_df = df_train, valid_df = df_val, path = \"\")\n",
        "    return data_lm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPli93VkVCpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract level 1 categorization from aspects\n",
        "def extract_class(aspect_str):\n",
        "    return str(aspect_str.split('/')[0])\n",
        "\n",
        "# convert float point sentiment scores to categories\n",
        "def score_to_class(sent_score):\n",
        "    if sent_score > '0':\n",
        "      return 'positive'\n",
        "    elif sent_score < '0':\n",
        "      return 'negative'\n",
        "    else:\n",
        "      return \"neutral\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgMNNRRyIH5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create datasets for Aspect Based Sentiment Classification (ABSA) on FiQA corpus\n",
        "\n",
        "def aspect_senti_sets():\n",
        "  with open('task1_headline_ABSA_train.json', 'r') as infile:  \n",
        "    dict_hdln = json.load(infile)\n",
        "  \n",
        "  pd_list_sent = []\n",
        "  pd_list_snip = []\n",
        "  pd_list_tar = []\n",
        "  pd_list_scr = []\n",
        "  pd_list_asp = []\n",
        "  for k in dict_hdln.keys():\n",
        "      pd_list_sent.append([k, dict_hdln[k]['sentence']])\n",
        "      pd_list_snip.append([k, dict_hdln[k]['info'][0]['snippets']])\n",
        "      pd_list_tar.append([k, dict_hdln[k]['info'][0]['target']])\n",
        "      pd_list_scr.append([k, dict_hdln[k]['info'][0]['sentiment_score']])\n",
        "      pd_list_asp.append([k, dict_hdln[k]['info'][0]['aspects']])\n",
        "  df_hdln_sent = pd.DataFrame(pd_list_sent,columns=['ID','sentence'])\n",
        "  df_hdln_snip = pd.DataFrame(pd_list_snip,columns=['ID','info:snippets'])\n",
        "  df_hdln_tar = pd.DataFrame(pd_list_tar,columns=['ID','info:target'])\n",
        "  df_hdln_scr = pd.DataFrame(pd_list_scr,columns=['ID','info:sentiment_score'])\n",
        "  df_hdln_asp = pd.DataFrame(pd_list_asp,columns=['ID','info:aspects'])\n",
        "  \n",
        "  pd_hdln_1 = pd.concat([df_hdln_sent,df_hdln_snip.iloc[:,1]],axis=1)\n",
        "  pd_hdln_2 = pd.concat([pd_hdln_1,df_hdln_tar.iloc[:,1]],axis=1)\n",
        "  pd_hdln_3 = pd.concat([pd_hdln_2,df_hdln_scr.iloc[:,1]],axis=1)\n",
        "  df_hdln = pd.concat([pd_hdln_3,df_hdln_asp.iloc[:,1]],axis=1)\n",
        "  \n",
        "  df_hdln['info:aspects'] = df_hdln['info:aspects'].str.strip('[]')\n",
        "  df_hdln['info:aspects'] = df_hdln['info:aspects'].str.strip('\\'\\'').astype('str')\n",
        "  \n",
        "  df_hdln['info_aspect_class'] = df_hdln['info:aspects'].apply(extract_class)\n",
        "  \n",
        "  # tokenization \n",
        "  tokenized_doc = df_hdln['sentence'].apply(lambda x: x.split())\n",
        "\n",
        "  # remove stop-words \n",
        "  tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "\n",
        "  # de-tokenization \n",
        "  detokenized_doc = [] \n",
        "  for i in range(len(df_hdln)): \n",
        "      t = ' '.join(tokenized_doc[i]) \n",
        "      detokenized_doc.append(t) \n",
        "\n",
        "  df_hdln['sentence'] = detokenized_doc\n",
        "  \n",
        "  # Building the aspect dataset\n",
        "  df_fiqa_hdln_aspect = pd.DataFrame({'label':df_hdln.info_aspect_class, 'text':df_hdln.sentence})\n",
        "  \n",
        "  # Building the sentiment dataset\n",
        "  df_hdln = df_hdln.rename(columns = {'info:sentiment_score':'info_sentiment_score'})\n",
        "  df_fiqa_hln_sent = pd.DataFrame({'labelscore':df_hdln.info_sentiment_score, 'text':df_hdln.sentence})\n",
        "  df_fiqa_hln_sent['labelscore'] = df_fiqa_hln_sent['labelscore'].fillna('0.0')\n",
        "  df_fiqa_hln_sent['label'] = df_fiqa_hln_sent['labelscore'].apply(score_to_class)\n",
        "  df_fiqa_hln_sent = df_fiqa_hln_sent[['label', 'text', 'labelscore']]\n",
        "  df_fiqa_hln_sent = df_fiqa_hln_sent.drop(columns = ['labelscore'])\n",
        "  \n",
        "  df_fiqa_hdln_aspect = df_fiqa_hdln_aspect.reset_index(drop=True)\n",
        "  df_fiqa_hln_sent = df_fiqa_hln_sent.reset_index(drop=True)\n",
        "  \n",
        "  return df_fiqa_hdln_aspect, df_fiqa_hln_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klSj0k_JbvpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_textclassdatabunch():\n",
        "    data_lm = get_finetuned_LM()\n",
        "    \n",
        "    df_fiqa_hdln_aspect, df_fiqa_hdln_sent = aspect_senti_sets()\n",
        "    \n",
        "    # split data into training and validation set\n",
        "    df_trn_aspect, df_val_aspect = train_test_split(df_fiqa_hdln_aspect, stratify = df_fiqa_hdln_aspect['label'], test_size = 0.3, random_state = 12)\n",
        "    df_trn_sent, df_val_sent = train_test_split(df_fiqa_hdln_sent, stratify = df_fiqa_hdln_sent['label'], test_size = 0.2, random_state = 12)\n",
        "    \n",
        "    # Aspect classifier model data\n",
        "    data_clas1 = TextClasDataBunch.from_df(path = \"\", train_df = df_trn_aspect, valid_df = df_val_aspect, vocab=data_lm.train_ds.vocab, bs=32)\n",
        "    \n",
        "    # Sentiment classifier model data\n",
        "    data_clas2 = TextClasDataBunch.from_df(path = \"\", train_df = df_trn_sent, valid_df = df_val_sent, vocab=data_lm.train_ds.vocab, bs=32)\n",
        "    \n",
        "    return data_lm, data_clas1, data_clas2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up0ziZqXxJur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_finetuned_aspect_classifier():\n",
        "    \n",
        "    data_lm, data_clas1, data_clas2 = get_textclassdatabunch()\n",
        "    \n",
        "    learn_aspect = language_model_learner(data_lm, arch=AWD_LSTM, drop_mult=0.7)\n",
        "    # train the learner object with learning rate = 1e-2\n",
        "    learn_aspect.fit_one_cycle(4, 1e-2, moms=(0.8, 0.7))\n",
        "    learn_aspect.save_encoder('ft_enc')\n",
        "    learn_aspect = text_classifier_learner(data_clas1, arch=AWD_LSTM, drop_mult=0.7)\n",
        "    learn_aspect.load_encoder('ft_enc')\n",
        "    learn_aspect.fit_one_cycle(1, 1e-2, moms=(0.8, 0.7))\n",
        "    learn_aspect.freeze_to(-2)\n",
        "    learn_aspect.fit_one_cycle(1, slice(5e-3/2., 5e-3), moms=(0.8, 0.7))\n",
        "    learn_aspect.unfreeze()\n",
        "    learn_aspect.fit_one_cycle(1, slice(2e-3/100, 2e-3), moms=(0.8, 0.7))\n",
        "    \n",
        "    return learn_aspect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc3WjcYQcoTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_finetuned_senti_classifier():\n",
        "    \n",
        "    data_lm, data_clas1, data_clas2 = get_textclassdatabunch()\n",
        "    \n",
        "    learn_senti = language_model_learner(data_lm, arch=AWD_LSTM, drop_mult=0.7)\n",
        "    # train the learner object with learning rate = 1e-2\n",
        "    learn_senti.fit_one_cycle(4, 1e-2, moms=(0.8, 0.7))\n",
        "    learn_senti.save_encoder('ft_enc')\n",
        "    learn_senti = text_classifier_learner(data_clas2, arch=AWD_LSTM, drop_mult=0.7)\n",
        "    learn_senti.load_encoder('ft_enc')\n",
        "    learn_senti.fit_one_cycle(4, 1e-2, moms=(0.8, 0.7))\n",
        "    learn_senti.freeze_to(-2)\n",
        "    learn_senti.fit_one_cycle(4, slice(5e-3/2., 5e-3), moms=(0.8, 0.7))\n",
        "    learn_senti.unfreeze()\n",
        "    learn_senti.fit_one_cycle(4, slice(2e-3/100, 2e-3), moms=(0.8, 0.7))\n",
        "    \n",
        "    return learn_senti\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C_fXnEyJPan",
        "colab_type": "code",
        "outputId": "3e85dd49-7448-42a3-b4d9-ff6fbd19221b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "learn_aspect = get_finetuned_aspect_classifier()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.056133</td>\n",
              "      <td>3.492874</td>\n",
              "      <td>0.369269</td>\n",
              "      <td>02:52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.587591</td>\n",
              "      <td>3.259112</td>\n",
              "      <td>0.390728</td>\n",
              "      <td>02:52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.328302</td>\n",
              "      <td>3.176452</td>\n",
              "      <td>0.399958</td>\n",
              "      <td>02:52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.200045</td>\n",
              "      <td>3.158475</td>\n",
              "      <td>0.402593</td>\n",
              "      <td>02:52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.401340</td>\n",
              "      <td>1.315203</td>\n",
              "      <td>0.381679</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.250894</td>\n",
              "      <td>1.220028</td>\n",
              "      <td>0.580153</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.164611</td>\n",
              "      <td>1.192525</td>\n",
              "      <td>0.633588</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQf_TVr6c1HC",
        "colab_type": "code",
        "outputId": "40d9b20e-b444-42ee-9785-d443d5a43642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        }
      },
      "source": [
        "learn_senti = get_finetuned_senti_classifier()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.056039</td>\n",
              "      <td>3.483662</td>\n",
              "      <td>0.371129</td>\n",
              "      <td>02:52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.582258</td>\n",
              "      <td>3.259999</td>\n",
              "      <td>0.391676</td>\n",
              "      <td>02:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.328262</td>\n",
              "      <td>3.179625</td>\n",
              "      <td>0.399691</td>\n",
              "      <td>02:54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.193767</td>\n",
              "      <td>3.161952</td>\n",
              "      <td>0.402176</td>\n",
              "      <td>02:53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.140165</td>\n",
              "      <td>1.019116</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.035617</td>\n",
              "      <td>0.788968</td>\n",
              "      <td>0.647727</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.942678</td>\n",
              "      <td>0.744092</td>\n",
              "      <td>0.647727</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.875791</td>\n",
              "      <td>0.739287</td>\n",
              "      <td>0.659091</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.720607</td>\n",
              "      <td>0.715160</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.726044</td>\n",
              "      <td>0.692565</td>\n",
              "      <td>0.693182</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.706991</td>\n",
              "      <td>0.692791</td>\n",
              "      <td>0.693182</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.672488</td>\n",
              "      <td>0.681399</td>\n",
              "      <td>0.693182</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.650281</td>\n",
              "      <td>0.672329</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.623020</td>\n",
              "      <td>0.662663</td>\n",
              "      <td>0.693182</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.609017</td>\n",
              "      <td>0.656776</td>\n",
              "      <td>0.670455</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.605527</td>\n",
              "      <td>0.656167</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhhKBIRykkHU",
        "colab_type": "code",
        "outputId": "0726e259-e9c3-493d-8014-ed1b5788daaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn_aspect.predict(\"Standard Chartered expected to pay just over $1 billion to resolve U.S., U.K. probes\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category Corporate, tensor(0), tensor([0.3373, 0.1855, 0.2541, 0.2230]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yHcTFlGJkCD",
        "colab_type": "code",
        "outputId": "8a1cfe17-ab45-4ff3-8e0c-7db64b30c76e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn_aspect.predict(\"Tesco leads leap in FTSE 100\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category Stock, tensor(3), tensor([0.2615, 0.1702, 0.2444, 0.3239]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0wvrds259bB",
        "colab_type": "code",
        "outputId": "84fc3333-f612-42b3-e56e-2313d60d0a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn_senti.predict(\"Royal Mail chairman Donald Brydon set to step down\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category negative, tensor(0), tensor([0.6005, 0.0096, 0.3900]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9zp_9zHJuGk",
        "colab_type": "code",
        "outputId": "4cab6aa9-b377-42a3-aa0f-e5eac0408b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn_senti.predict(\"Tesco leads leap in FTSE 100\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category positive, tensor(2), tensor([0.0782, 0.0051, 0.9167]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}